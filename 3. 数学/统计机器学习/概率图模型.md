
## 从统计推断的角度来看概率图模型：

首先，绝大多数的统计机器学习本质上是在进行推断：根据已有的样本和标签，获得对于未知标签的样本的**推断**。
要解释一下**推断**的话，基本可以理解为获得**样本与标签之间的联系。**
区分得再细一些，获得推断的方式很多，我们采用概率模型——概率模型的出发点是，获得随机变量的概率分布的认识。
说得再数学一些，我们希望得到对于$$
p(\boldsymbol{y} \mid \boldsymbol{x}, \boldsymbol{\theta}) \text { 或 } p(\boldsymbol{y} \mid \boldsymbol{x})
$$
的认识。

对统计推断的解决方案基本上可以不重不漏的分为两种：**生成模型** 和 **判别模型。**
这两点从物理上是有直观理解的，前者我们是要考虑模型的生成机制，往往是可解释的，而判别模型则不关注生成的机制（或者说唯象地考虑样本的情况）。

这两点的区分从数学上应该可以说得更清楚：前者直接考虑联合分布
$$
p(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{\theta})
$$
然后通过样本之间的依赖关系（独立性或者类似概率图的依赖关系）（重要的是$x_i$彼此之间的依赖）来获得条件分布
$$
p(\boldsymbol{y} \mid \boldsymbol{x})
$$

后者则是直接对于$$
p(\boldsymbol{y} \mid \boldsymbol{x})
$$建模，例如对logistic回归模型就是：
$$
p(y \mid \boldsymbol{x}, \boldsymbol{\theta})=\frac{1}{1+\mathrm{e}^{-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x}}}
$$

好了，有了这些看法，我们接下来要讨论的概率图模型就有落脚点了：这是一种非常常用的生成模型。

基本上来说，我们目前了解的生成模型只有两种：朴素贝叶斯，以及概率图模型（这里面的内容很多，因为不同类型的图对应不同的模型，例如隐马尔可夫模型——以及衍生的自回归隐马尔科夫模型、可分解隐马尔科夫模型）

再进一步——概率图模型又可以分为有向图（以HMM为代表），以及无向图（以马尔科夫随机场为代表）,以图模型是否有边作为区分——

表明了数据依赖之间的方向性，例如有向图往往对应于时间序列，而无向图则适合处理图像等类型的数据。


<!--stackedit_data:
eyJoaXN0b3J5IjpbMTk2NTU5NTEyMF19
-->