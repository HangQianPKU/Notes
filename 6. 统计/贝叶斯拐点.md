### 1.1 以线性回归为例引入贝叶斯统计
$$
\begin{aligned}
L(D, \vec{\beta}) & =\|X \vec{\beta}-Y\|^2 \\
& =(X \vec{\beta}-Y)^{\top}(X \vec{\beta}-Y) \\
& =Y^{\top} Y-Y^{\top} X \vec{\beta}-\vec{\beta}^{\top} X^{\top} Y+\vec{\beta}^{\top} X^{\top} X \vec{\beta}
\end{aligned}
$$

As the loss is convex the optimum solution lies at gradient zero. The gradient of the loss function is (using Denominator layout convention):
$$
\begin{aligned}
\frac{\partial L(D, \vec{\beta})}{\partial \vec{\beta}} & =\frac{\partial\left(Y^{\top} Y-Y^{\top} X \vec{\beta}-\vec{\beta}^{\top} X^{\top} Y+\vec{\beta}^{\top} X^{\top} X \vec{\beta}\right)}{\partial \vec{\beta}} \\
& =-2 X^{\top} Y+2 X^{\top} X \vec{\beta}
\end{aligned}
$$

Setting the gradient to zero produces the optimum parameter:
$$
\begin{aligned}
-2 X^{\top} Y+2 X^{\top} X \vec{\beta} & =0 \\
\Rightarrow X^{\top} X \vec{\beta} & =X^{\top} Y \\
\Rightarrow \overrightarrow{\hat{\beta}} & =\left(X^{\top} X\right)^{-1} X^{\top} Y
\end{aligned}
$$

### 1.2 贝叶斯公式

$$
\begin{aligned}
\mathbb{P}(A|B) = \frac{\mathbb{P}(AB)}{\mathbb{P}(B)} = \frac{\mathbb{P}(A)\mathbb{P}(B|A)}{\mathbb{P}(B)}
\end{aligned}
$$

$$
\begin{aligned}
\mathbb{P}(\theta|Y) = \frac{\mathbb{P}(\theta Y)}{\mathbb{P}(Y)} = \frac{\mathbb{P}(\theta)\mathbb{P}(Y|\theta)}{\mathbb{P}(Y)}
\end{aligned}
$$

$$
\begin{aligned}
\underbrace{\mathbb{P}(\theta | Y)}_{\text{后验概率(在获得了模型的样本后，对参数的新认识)}} = \underbrace{\mathbb{P}(\theta)}_{\text{先验概率——对模型参数的先验认识}} · \underbrace{\mathbb{P}(Y|\theta)}_{\text{似然概率——模型}}/\mathbb{P}(Y)
\end{aligned}
$$

$$
(2 \pi)^{-k / 2} \operatorname{det}(\boldsymbol{\Sigma})^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)
$$
### 3. 公式推导
>**引理一**
设$\mathbf{Y} = (Y_1, Y_2,...,Y_n)$是从泊松分布Poisson分布Poi($\lambda$)中抽样的随机样本，其中$\lambda$作为贝叶斯模型下的模型参数服从Gamma分布：Gamma$(\alpha, \beta)$,下面证明$\lambda$的后延分布是Gamma$(\Sigma_{k=i}^{j}Y_k+\alpha,n+\beta)$

证明：
$$
\begin{align}
f(\lambda|\mathbf{Y}) & \propto P(\mathbf{Y}|\lambda)·f(\lambda) 
\\& \propto \Pi_{k=i}^{j} (\frac{\lambda^{Y_k}}{Y_k}e^{-\lambda})
·\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{-\beta \lambda}
\\& \propto \lambda^{\Sigma_{k=i}^{j}Y_k}e^{-n\lambda} · \lambda^{\alpha-1}e^{-\beta \lambda}
\\& \propto \lambda^{\alpha+\Sigma_{k=i}^{j}Y_k-1} e ^{-(n+\beta)\lambda}
\bm{\sim} Gamma(\alpha+\Sigma_{k=i}^{j}Y_k, n+\beta)
\end{align}
$$

>**Calculation Step1.1 无拐点边缘分布**
>设$\mathbf{Y} = (Y_1, Y_2,...,Y_n)$是从泊松分布Poisson$(\lambda)$中抽样的随机样本，其中$\lambda$作为贝叶斯模型下的模型参数服从Gamma分布：Gamma$(\alpha, \beta)$，则$\mathbf{Y}$的边际分布$\mathbb{P}(\mathbf{Y}) = \int_{\lambda=0}^{+\infin}\mathbb{P}(\mathbf{Y}|\lambda)f(\lambda)d\lambda$可以计算，结果是：

$$
\begin{align}
\mathbb{P}(\mathbf{Y})  
& =  \int_{0}^{+\infin}\mathbb{P}(\mathbf{Y}|\lambda)f(\lambda)d\lambda
\\& = \int_{0}^{+\infin} \Pi_{k=i}^{j} (\frac{\lambda^{Y_k}}{Y_k}e^{-\lambda})
·\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{\beta \lambda} d\lambda
\\& = \underbrace{\Pi_{k=i}^{j}\frac{1}{Y_k!}·\frac{\beta^\alpha}{\Gamma(\alpha)}}_{\text{constant part}} ·\underbrace{\int_{0}^{+\infin}\lambda^{\Sigma_{k=i}^{j}Y_k+\alpha-1}·e^{-(n+\beta)\lambda}d\lambda}_{\text{terms containing}\lambda\text{——kernel of }Gamma(\Sigma_{k=i}^{j}Y_k+\alpha,n+\beta)}
\\& =\underbrace{\Pi_{k=i}^{j}\frac{1}{Y_k!}·\frac{\beta^\alpha}{\Gamma(\alpha)}}_{\text{constant part}} · \underbrace{\frac{\Gamma(\Sigma_{k=i}^{j}y_k+\alpha)}{(n+\beta)^{(\Sigma_{k=i}^{j}y_k+\alpha)}}}_{\text{integral of kernel part:Gamma}(\Sigma_{k=i}^{j}Y_k+\alpha,n+\beta)}
\end{align}
$$

>**Calculation Step1.2 拐点动态规划的Chapman-Kolmogorov演化方程**

- k=1的情形
$$
\begin{align}
&P(Y_{0:N-1} |k=1) \\ 
&=\Sigma_{c_1 = 1}^{N-2}
[P(Y_{0:N-1} | c_1, k=1)
P(c_1|k=1)] \\
&=\Sigma_{c_1 = 1}^{N-2}
[P(Y_{0:N-1} | c_1, k=1)
/\binom{N-2}{1}] \\
&=\Sigma_{c_1 = 1}^{N-2}
[P(Y_{0:c_1} | k=0)
P(Y_{c_1+1:N-1} | k=0)
/\binom{N-2}{1}] \\
&= \frac{1}{\binom{N-2}{1}}
\Sigma_{c_1 = 1}^{N-2}
[P(Y_{0:c_1} | k=0)
P(Y_{c_1+1:N-1} | k=0)]
\end{align} 
$$
- k=2的情形：
$$
\begin{align}
&P(Y_{0:N-1} | k=2) \\
&=\Sigma_{c_2=2}^{(N-1)-1}
\Sigma_{c_1=1}^{c_2-1}
P(Y_{1:N-1}|c_1,c_2,k=2)
P(c_1,c_2|k=2) \\
&= \frac{1}{\binom{N-2}{2}} 
\Sigma_{c_2=2}^{N-2}
[\Sigma_{c_1=1}^{c_2-1}
P(Y_{1:N-1}|c_1,c_2,k=2)] \\
&= \frac{1}{\binom{N-2}{2}} 
\Sigma_{c_2=2}^{N-2}
[\Sigma_{c_1=1}^{c_2-1}
P(Y_{1:c_2}|c_1,k=1)
P(Y_{c_2+1:N-1}|k=0)
] \\
&= \frac{1}{\binom{N-2}{2}} 
\Sigma_{c_2=2}^{N-2}
[
P(Y_{c_2+1:N-1}|k=0)
\Sigma_{c_1=1}^{c_2-1}
P(Y_{1:c_2}|c_1,k=1) \\
&= \frac{1}{\binom{N-2}{2}} 
\Sigma_{c_2=2}^{N-2}
[
P(Y_{c_2+1:N-1}|k=0)
\frac{P(Y_{1:c_2}|k=1)}{1/\binom{N-2}{1}}
] \\
&= \frac{\binom{N-2}{1}}{\binom{N-2}{2}} 
[
\Sigma_{c_2=2}^{N-2}
P(Y_{1:c_2}|k=1)
P(Y_{c_2+1:N-1}|k=0)
]
\end{align}
$$
- k>=2的情形,归纳可得：
$$
\begin{align}
&P(Y_{0:N-1}|k) \\
&= \frac{\binom{N-1}{k-1}}{\binom{N-1}{k}}
[
\Sigma_{c_k=k}^{N-2}
P(Y_{1:c_k}|k-1)
P(Y_{c_k+1:N-1}|k=0)
]
\end{align}
$$


>Sample Step1

- 采样顺序：
拐点个数——拐点位置——每个时间段内的速率
其中拐点位置从后向前
- 代码实现
分成三类： $k \in \{2,3,...k_{max}\}$,$k=1$,$k=0$
- 后验概率计算
>Step 6.1 拐点个数

$$
P(k | Y) = f_k = \frac{P_k(Y_{1:N})·f(K=k)}{f(Y_{1:N})}\\ =\frac{P_k(Y_{1:N})·f(K=k)}{\Sigma_kP_k(Y_{1:N})·f(K=k)}
$$
>Step 6.2 拐点分布
>
$$
P(c_k | c_{k+1}) = \frac{\mathbb{P_{k-1}}(Y_{1:c_k})f(Y_{c_k+1:c_{k+1}})}{\Sigma_{v \in[k-1, c_{k+1})}\mathbb{P_{k-1}}(Y_{1:v})f(Y_{v+1:c_{k+1}})}
$$
>Step 6.3 Regression parameter

rate parameter不用$\lambda$的原因是Python中（事实上大部分的编程语言都是这样）的lambda关键字被函数式编程范式中的lambda表达式占用了

$$
P(\mu | Y) \propto P(Y|\mu)P(\mu) \\ \propto \Pi_k (\mu^{y_k}·e^{-\mu})·\mu^{\alpha-1}e^{\beta \mu} \\ 
=\mu^{(\Sigma_k y_k +\alpha -1)}e^{-(n+\beta)\mu} 
$$
which is  $$Gamma(\Sigma_k y_k +\alpha ,n+\beta)$$



>**引理二**
设$\mathbf{Y} = (Y_1, Y_2,...,Y_n)$是从负二项分布NegativeBinomial Nb(r,p)中抽样的随机样本，其中我们认为$r$已知，$p$作为贝叶斯模型下的模型参数服从Beta分布：Beta$(\alpha, \beta)$,下面证明$p$的后延分布是~~Beta($\alpha+rn, \Sigma_{j=1}^{n}Y_j-nr+\beta)$~~, Beta$(\alpha+rn, \Sigma_{j=1}^{n}Y_j+\beta)$

>~~韦来生的贝叶斯统计书习题三12题给出的结论是上面写的，但我算出来是没有那项-nr的，两者不同~~
其实是定义不同，书上对负二项分布的定义是：
$$
f(x|\theta) = P(\bm{X}=x |\theta) = \binom{x-1}{r-1}\theta^{r}(1-\theta)^{x-r}, x=r,r+1,···
$$


证明：
$$
\begin{align}
f(p|\mathbf{Y}) & \propto P(\mathbf{Y}|r,p)·f(p) 
\\& \propto \Pi_{j=1}^{n}(\binom{-r}{Y_j}(1-p)^{Y_j}p^r) · \frac{1}{B(\alpha,\beta)}p^{\alpha-1}(1-p)^{\beta-1} 
\\& \propto (1-p)^{\Sigma_{j=1}^{n}Y_{j}} p^{nr} · p^{\alpha-1}(1-p)^{\beta-1}
\\& \propto p^{\alpha+nr-1} (1-p)^{\Sigma_{j=1}^{n}Y_j+\beta-1} \bm{\sim} Be(\alpha+nr, \Sigma_{j=1}^{n}Y_j+\beta)
\end{align}
$$
>Calculation Step1 公式推导
>设$\mathbf{Y} = (Y_1, Y_2,...,Y_n)$是从负二项分布NegativeBinomial Nb(r,p)中抽样的随机样本，其中我们认为$r$已知，$p$作为贝叶斯模型下的模型参数服从Beta分布：Beta$(\alpha, \beta)$，则$\mathbf{Y}$的边际分布$\mathbb{P}(\mathbf{Y}) = \int_{p=0}^{+\infin}\mathbb{P}(\mathbf{Y}|p)f(p)dp$可以计算，结果是：

$$
\begin{align}
\mathbb{P}(\mathbf{Y})  
& =  \int_{0}^{+\infin}\mathbb{P}(\mathbf{Y}|p)f(p)dp
\\& =\int_{0}^{+\infin} \Pi_{j=1}^{n}(\binom{-r}{Y_j}(1-p)^{Y_j}p^r) · \frac{1}{B(\alpha,\beta)}p^{\alpha-1}(1-p)^{\beta-1} dp
\\& = \underbrace{\frac{\Pi_{j=1}^n \binom{-r}{Y_j}}{B(\alpha,\beta)}}_{\text{constant part}} · \underbrace{\int_{0}^{+\infin }p^{nr+\alpha-1} (1-p)^{\beta+\Sigma_{j=1}^{n}Y_j-1}}_{\text{terms containing p——kernel of }Beta(\alpha+nr,\beta+\Sigma_{j=1}^{n}Y_j)}
\\& =\underbrace{\frac{\Pi_{j=1}^n \binom{-r}{Y_j}}{B(\alpha,\beta)}}_{\text{constant part}} · \underbrace{B(\alpha+nr,\beta+\Sigma_{j=1}^{n}Y_j)}_{\text{integral of kernel part:Beta}(\alpha+nr, \beta+\Sigma_{j=1}^{n}Y_j)}
\end{align}
$$


### NegativeBinomial Property
>**Property 1** Negative Binomial Distribution as **Poisson-Gamma Mixture Distribution**

$$
\begin{align}
P& = \int_{0}^{+\infty}f_{\text{Poi}}(\lambda)f_{\text{Gamma}}(r,\frac{p}{1-p})(\lambda)d\lambda
\\& =\int_{0}^{+\infty}(\frac{\lambda ^k}{k!}e^{-\lambda})[\frac{1}{\Gamma(r)}(\frac{p}{1-p})^r\lambda^{r-1}e^{-\frac{p}{1-p}\lambda}]d\lambda
\\& = (\frac{p}{1-p})^r \frac{1}{k!·\Gamma(r)}\int_0^{+\infty} \lambda^{r+k-1} \text{exp}(-\lambda(\frac{1-p+p}{1-p}))d\lambda
\\& = (\frac{p}{1-p})^r \frac{1}{k!·\Gamma(r)}\int_0^{+\infty} \lambda^{r+k-1} \text{exp}(-\lambda\frac{1}{1-p})d\lambda
\\& = (\frac{p}{1-p})^r \frac{1}{k!·\Gamma(r)} · \frac{\Gamma(r+k)}{[1/(1-p)]^{r+k}}
\\& = \frac{\Gamma(r+k)}{\Gamma(r)\Gamma(k+1)}p^r(1-p)^k
\\& = \binom{k+r-1}{k}p^r(1-p)^k
\\& = \binom{-r}{k}p^r(1-p)^k
\end{align}
$$


> **Property 2**
> Property of Negative Binimial Distribution
> $\mathbb{E}(X) = r\frac{1-p}{p} = r(1/p-1)$
> $\mathbb{Var}(X) = r\frac{1-p}{p^2}$
> $\frac{\mathbb{Var}(X)}{\mathbb{E}(X)} = \frac{1}{p}$

> **Property 3 NB as Compound Poisson Distribution**

The negative binomial distribution $\mathrm{NB}\left(r, p\right)$ can be represented as a compound Poisson distribution: Let $\left\{Y_n\}, n \in \mathbb{N}\right.$ denote a sequence of independent and identically distributed random variables, each one having the **logarithmic series distribution** $\log (p)$, with probability mass function
$$
f(k ; r, p)=\frac{p^k}{k \ln (1/1-p)}, \quad k \in \mathbb{N} .
$$

Let $N$ be a random variable, independent of the sequence, and suppose that $N$ has a Poisson distribution with mean $\lambda=-r \ln (1-p)$. Then the random sum
$$
X=\sum_{n=1}^N Y_n
$$
is $\mathrm{NB}(r, p)$-distributed. 

To prove this, we calculate the probability generating function $G_X$ of $X$, which is the composition of the probability generating functions $G_N$ and $G_{Y_1}$. Using
$$
G_N(z)=\exp (\lambda(z-1)), \quad z \in \mathbb{R},
$$
and
$$
G_{Y_1}(z)=\frac{\ln (1-p z)}{\ln (1-p)}, \quad|z|<\frac{1}{p},
$$
we obtain
$$
\begin{align}
G_X(z) & =G_N\left(G_{Y_1}(z)\right) \\
& =\exp \left(\lambda\left(\frac{\ln (1-p z)}{\ln (1-p)}-1\right)\right) \\
& =\exp (-r(\ln (1-p z)-\ln (1-p))) \\
& =\left(\frac{1-p}{1-p z}\right)^r, \quad|z|<\frac{1}{p}
\end{align}
$$

>**Property 4 Relation with Poisson**

consider a sequence of negative binomial random variables where the stopping parameter r goes to infinity, while the probability p of success in each trial goes to one, in such a way as to keep the mean of the distribution (i.e. the expected number of failures) constant. Denoting this mean as $\lambda$, the parameter p will be $p=r /(r+\lambda)$ Mean: $\lambda=\frac{(1-p) r}{p} \Rightarrow p=\frac{r}{r+\lambda}$, Variance: $\lambda\left(1+\frac{\lambda}{r}\right)>\lambda$, thus always overdispersed.

Under this parametrization the probability mass function will be

$$  
f(k ; r, p)=\frac{\Gamma(k+r)}{k ! \cdot \Gamma(r)}(1-p)^k p^r=\frac{\lambda^k}{k !} \cdot \frac{\Gamma(r+k)}{\Gamma(r)(r+\lambda)^k} \cdot \frac{1}{\left(1+\frac{\lambda}{r}\right)^r}  
$$

  

Now if we consider the limit as $r \rightarrow \infty$, the second factor will converge to one, and the third to the exponent function:

$$  
\lim _{r \rightarrow \infty} f(k ; r, p)=\frac{\lambda^k}{k !} \cdot 1 \cdot \frac{1}{e^\lambda}  
$$

  

which is the mass function of a Poisson-distributed random variable with expected value \lambda. In other words, the alternatively parameterized negative binomial distribution converges to the Poisson distribution and r controls the deviation from the Poisson. This makes the negative binomial distribution suitable as a robust alternative to the Poisson, which approaches the Poisson for large r, but which has larger variance than the Poisson for small r.

$$  
\operatorname{Poisson}(\lambda)=\lim _{r \rightarrow \infty} \mathrm{NB}\left(r, \frac{r}{r+\lambda}\right) .  
$$

  

计算该概率密度的函数放在/src_test/nochangepoints.py中


- 231014中的step 6.3 对应的 Negative_Binomial版本
>Step 6.3 Regression parameter

$$
\begin{align}
\mathbb{P}(p|\mathbf{Y}) 
& \propto \mathbb{P}(\mathbf{Y}|p)\mathbb{P}(p)
\\& \propto \Pi_{j}^{}(\binom{-r}{Y_j}(1-p)^{Y_j}p^r) · \frac{1}{B(\alpha,\beta)}p^{\alpha-1}(1-p)^{\beta-1})
\\& \propto \Pi_{j}^{}[(1-p)^{Y_j}p^r] ·  p^{\alpha-1}(1-p)^{\beta-1}
\\& \propto p^{nr +\alpha-1}(1-p)^{\Sigma_{j}Y_j +\beta-1}
\end{align} 
$$
which is 
$$
Beta(\alpha+nr, \beta+\Sigma_{j}Y_j)
$$

### NegativeBinomial 对应的物理
>Definition

$N$: **某时间段内**火山事件的规模(单位：次)
$Y_j$: **某时间段内**第j次火山事件发生后产生并保留下来被采集到的锆石个数（单位：个数/次）
$X=\sum_{j=1}^N Y_j$: **某段时间内**产生并保留下来的锆石个数（单位：个数）

$p$: 与$\frac{\text{subduction}}{\text{collision}}$正相关的量，该值越**大**，表明某一时间段内全球**正在发生火山事件的构造环境**中subduction占比**多**，collision占比**少**, preservation potential**低**

$$
Y_j \sim \text{LogSeries}(1-p)
$$

$$
\underbrace{\mathbb{E}(Y_j)}_{\searrow} = \underbrace{\frac{1}{ln(1/(1-p)}}_{\searrow}· \underbrace{\frac{1-p}{p}}_{\searrow} \quad \text{as} \quad p\nearrow
$$

$$
N \sim \text{Poisson}(\lambda = rln(\frac{1}{1-p}))
$$

$$
\mathbb{E}(N)= r· \underbrace{ln(\frac{1}{1-p})}_{\nearrow} \quad as \quad p \nearrow
$$

$$
X=\sum_{j=1}^N Y_j
$$
so
$$
\mathbb{E}(X) = \mathbb{E}(N)·\mathbb{E}(Y_j) = \underbrace{r\frac{1-p}{p}}_{\searrow} \quad \text{as} \quad p \nearrow 
$$

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTk3ODg4MjMwMywxNzk3ODA4ODU2LC0xOT
g0ODQxMDg1LDE0NjY1OTkxOTYsLTEzMjkyMTE5NzQsMTI5ODUz
MjI0NiwtNjE0NTE1OTA1LC0yMTA3MTc4NjgyLDE3NTE1MjA2OC
wtODQyNTQ1NDczLC0xNTQyNDE4MzEyXX0=
-->