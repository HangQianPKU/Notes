>**引理一**
设$\mathbf{Y} = (Y_1, Y_2,...,Y_n)$是从泊松分布Poisson分布Poi($\lambda$)中抽样的随机样本，其中$\lambda$作为贝叶斯模型下的模型参数服从Gamma分布：Gamma$(\alpha, \beta)$,下面证明$\lambda$的后延分布是Gamma$(\Sigma_{k=i}^{j}Y_k+\alpha,n+\beta)$

证明：
$$
\begin{align}
f(\lambda|\mathbf{Y}) & \propto P(\mathbf{Y}|\lambda)·f(\lambda) 
\\& \propto \Pi_{k=i}^{j} (\frac{\lambda^{Y_k}}{Y_k}e^{-\lambda})
·\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{-\beta \lambda}
\\& \propto \lambda^{\Sigma_{k=i}^{j}Y_k}e^{-n\lambda} · \lambda^{\alpha-1}e^{-\beta \lambda}
\\& \propto \lambda^{\alpha+\Sigma_{k=i}^{j}Y_k-1} e ^{-(n+\beta)\lambda}
\bm{\sim} Gamma(\alpha+\Sigma_{k=i}^{j}Y_k, n+\beta)
\end{align}
$$

>**Calculation Step1.1 无拐点边缘分布**
>设$\mathbf{Y} = (Y_1, Y_2,...,Y_n)$是从泊松分布Poisson$(\lambda)$中抽样的随机样本，其中$\lambda$作为贝叶斯模型下的模型参数服从Gamma分布：Gamma$(\alpha, \beta)$，则$\mathbf{Y}$的边际分布$\mathbb{P}(\mathbf{Y}) = \int_{\lambda=0}^{+\infin}\mathbb{P}(\mathbf{Y}|\lambda)f(\lambda)d\lambda$可以计算，结果是：

$$
\begin{align}
\mathbb{P}(\mathbf{Y})  
& =  \int_{0}^{+\infin}\mathbb{P}(\mathbf{Y}|\lambda)f(\lambda)d\lambda
\\& = \int_{0}^{+\infin} \Pi_{k=i}^{j} (\frac{\lambda^{Y_k}}{Y_k}e^{-\lambda})
·\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{\beta \lambda} d\lambda
\\& = \underbrace{\Pi_{k=i}^{j}\frac{1}{Y_k!}·\frac{\beta^\alpha}{\Gamma(\alpha)}}_{\text{constant part}} ·\underbrace{\int_{0}^{+\infin}\lambda^{\Sigma_{k=i}^{j}Y_k+\alpha-1}·e^{-(n+\beta)\lambda}d\lambda}_{\text{terms containing}\lambda\text{——kernel of }Gamma(\Sigma_{k=i}^{j}Y_k+\alpha,n+\beta)}
\\& =\underbrace{\Pi_{k=i}^{j}\frac{1}{Y_k!}·\frac{\beta^\alpha}{\Gamma(\alpha)}}_{\text{constant part}} · \underbrace{\frac{\Gamma(\Sigma_{k=i}^{j}y_k+\alpha)}{(n+\beta)^{(\Sigma_{k=i}^{j}y_k+\alpha)}}}_{\text{integral of kernel part:Gamma}(\Sigma_{k=i}^{j}Y_k+\alpha,n+\beta)}
\end{align}
$$

>**Calculation Step1.2 拐点动态规划的Chapman-Kolmogorov演化方程**

- k=1的情形
$$
\begin{align}
&P(Y_{0:N-1} |k=1) \\ 
&=\Sigma_{c_1 = 1}^{N-2}
[P(Y_{0:N-1} | c_1, k=1)
P(c_1|k=1)] \\
&=\Sigma_{c_1 = 1}^{N-2}
[P(Y_{0:N-1} | c_1, k=1)
/\binom{N-2}{1}] \\
&=\Sigma_{c_1 = 1}^{N-2}
[P(Y_{0:c_1} | k=0)
P(Y_{c_1+1:N-1} | k=0)
/\binom{N-2}{1}] \\
&= \frac{1}{\binom{N-2}{1}}
\Sigma_{c_1 = 1}^{N-2}
[P(Y_{0:c_1} | k=0)
P(Y_{c_1+1:N-1} | k=0)]
\end{align} 
$$
- k=2的情形：
$$
\begin{align}
&P(Y_{0:N-1} | k=2) \\
&=\Sigma_{c_2=2}^{(N-1)-1}
\Sigma_{c_1=1}^{c_2-1}
P(Y_{1:N-1}|c_1,c_2,k=2)
P(c_1,c_2|k=2) \\
&= \frac{1}{\binom{N-2}{2}} 
\Sigma_{c_2=2}^{N-2}
[\Sigma_{c_1=1}^{c_2-1}
P(Y_{1:N-1}|c_1,c_2,k=2)] \\
&= \frac{1}{\binom{N-2}{2}} 
\Sigma_{c_2=2}^{N-2}
[\Sigma_{c_1=1}^{c_2-1}
P(Y_{1:c_2}|c_1,k=1)
P(Y_{c_2+1:N-1}|k=0)
] \\
&= \frac{1}{\binom{N-2}{2}} 
\Sigma_{c_2=2}^{N-2}
[
P(Y_{c_2+1:N-1}|k=0)
\Sigma_{c_1=1}^{c_2-1}
P(Y_{1:c_2}|c_1,k=1) \\
&= \frac{1}{\binom{N-2}{2}} 
\Sigma_{c_2=2}^{N-2}
[
P(Y_{c_2+1:N-1}|k=0)
\frac{P(Y_{1:c_2}|k=1)}{1/\binom{N-2}{1}}
] \\
&= \frac{\binom{N-2}{1}}{\binom{N-2}{2}} 
[
\Sigma_{c_2=2}^{N-2}
P(Y_{1:c_2}|k=1)
P(Y_{c_2+1:N-1}|k=0)
]
\end{align}
$$
- k>=2的情形,归纳可得：
$$
\begin{align}
&P(Y_{0:N-1}|k) \\
&= \frac{\binom{N-1}{k-1}}{\binom{N-1}{k}}
[
\Sigma_{c_k=k}^{N-2}
P(Y_{1:c_k}|k-1)
P(Y_{c_k+1:N-1}|k=0)
]
\end{align}
$$



>**引理二**
设$\mathbf{Y} = (Y_1, Y_2,...,Y_n)$是从负二项分布NegativeBinomial Nb(r,p)中抽样的随机样本，其中我们认为$r$已知，$p$作为贝叶斯模型下的模型参数服从Beta分布：Beta$(\alpha, \beta)$,下面证明$p$的后延分布是~~Beta($\alpha+rn, \Sigma_{j=1}^{n}Y_j-nr+\beta)$~~, Beta$(\alpha+rn, \Sigma_{j=1}^{n}Y_j+\beta)$

>~~韦来生的贝叶斯统计书习题三12题给出的结论是上面写的，但我算出来是没有那项-nr的，两者不同~~
其实是定义不同，书上对负二项分布的定义是：
$$
f(x|\theta) = P(\bm{X}=x |\theta) = \binom{x-1}{r-1}\theta^{r}(1-\theta)^{x-r}, x=r,r+1,···
$$


证明：
$$
\begin{align}
f(p|\mathbf{Y}) & \propto P(\mathbf{Y}|r,p)·f(p) 
\\& \propto \Pi_{j=1}^{n}(\binom{-r}{Y_j}(1-p)^{Y_j}p^r) · \frac{1}{B(\alpha,\beta)}p^{\alpha-1}(1-p)^{\beta-1} 
\\& \propto (1-p)^{\Sigma_{j=1}^{n}Y_{j}} p^{nr} · p^{\alpha-1}(1-p)^{\beta-1}
\\& \propto p^{\alpha+nr-1} (1-p)^{\Sigma_{j=1}^{n}Y_j+\beta-1} \bm{\sim} Be(\alpha+nr, \Sigma_{j=1}^{n}Y_j+\beta)
\end{align}
$$

### NegativeBinomial Property
>**Property 1** Negative Binomial Distribution as **Poisson-Gamma Mixture Distribution**

$$
\begin{align}
P& = \int_{0}^{+\infty}f_{\text{Poi}}(\lambda)f_{\text{Gamma}}(r,\frac{p}{1-p})(\lambda)d\lambda
\\& =\int_{0}^{+\infty}(\frac{\lambda ^k}{k!}e^{-\lambda})[\frac{1}{\Gamma(r)}(\frac{p}{1-p})^r\lambda^{r-1}e^{-\frac{p}{1-p}\lambda}]d\lambda
\\& = (\frac{p}{1-p})^r \frac{1}{k!·\Gamma(r)}\int_0^{+\infty} \lambda^{r+k-1} \text{exp}(-\lambda(\frac{1-p+p}{1-p}))d\lambda
\\& = (\frac{p}{1-p})^r \frac{1}{k!·\Gamma(r)}\int_0^{+\infty} \lambda^{r+k-1} \text{exp}(-\lambda\frac{1}{1-p})d\lambda
\\& = (\frac{p}{1-p})^r \frac{1}{k!·\Gamma(r)} · \frac{\Gamma(r+k)}{[1/(1-p)]^{r+k}}
\\& = \frac{\Gamma(r+k)}{\Gamma(r)\Gamma(k+1)}p^r(1-p)^k
\\& = \binom{k+r-1}{k}p^r(1-p)^k
\\& = \binom{-r}{k}p^r(1-p)^k
\end{align}
$$


> **Property 2**
> Property of Negative Binimial Distribution
> $\mathbb{E}(X) = r\frac{1-p}{p} = r(1/p-1)$
> $\mathbb{Var}(X) = r\frac{1-p}{p^2}$
> $\frac{\mathbb{Var}(X)}{\mathbb{E}(X)} = \frac{1}{p}$

> **Property 3 NB as Compound Poisson**

The negative binomial distribution $\mathrm{NB}\left(r, p\right)$ can be represented as a compound Poisson distribution: Let $\left\{Y_n\}, n \in \mathbb{N}\right.$ denote a sequence of independent and identically distributed random variables, each one having the **logarithmic series distribution** $\log (p)$, with probability mass function
$$
f(k ; r, p)=\frac{-p^k}{k \ln (1-p)}, \quad k \in \mathbb{N} .
$$

Let $N$ be a random variable, independent of the sequence, and suppose that $N$ has a Poisson distribution with mean $\lambda=-r \ln (1-p)$. Then the random sum
$$
X=\sum_{n=1}^N Y_n
$$
is $\mathrm{NB}(r, p)$-distributed. 

To prove this, we calculate the probability generating function $G_X$ of $X$, which is the composition of the probability generating functions $G_N$ and $G_{Y_1}$. Using
$$
G_N(z)=\exp (\lambda(z-1)), \quad z \in \mathbb{R},
$$
and
$$
G_{Y_1}(z)=\frac{\ln (1-p z)}{\ln (1-p)}, \quad|z|<\frac{1}{p},
$$
we obtain
$$
\begin{aligned}
G_X(z) & =G_N\left(G_{Y_1}(z)\right) \\
& =\exp \left(\lambda\left(\frac{\ln (1-p z)}{\ln (1-p)}-1\right)\right) \\
& =\exp (-r(\ln (1-p z)-\ln (1-p))) \\
& =\left(\frac{1-p}{1-p z}\right)^r, \quad|z|<\frac{1}{p}
\end{aligned}
$$

>**Property 4 Relation with Poisson**

consider a sequence of negative binomial random variables where the stopping parameter r goes to infinity, while the probability p of success in each trial goes to one, in such a way as to keep the mean of the distribution (i.e. the expected number of failures) constant. Denoting this mean as $\lambda$, the parameter p will be $p=r /(r+\lambda)$ Mean: $\lambda=\frac{(1-p) r}{p} \Rightarrow p=\frac{r}{r+\lambda}$, Variance: $\lambda\left(1+\frac{\lambda}{r}\right)>\lambda$, thus always overdispersed.

Under this parametrization the probability mass function will be

$$  
f(k ; r, p)=\frac{\Gamma(k+r)}{k ! \cdot \Gamma(r)}(1-p)^k p^r=\frac{\lambda^k}{k !} \cdot \frac{\Gamma(r+k)}{\Gamma(r)(r+\lambda)^k} \cdot \frac{1}{\left(1+\frac{\lambda}{r}\right)^r}  
$$

  

Now if we consider the limit as $r \rightarrow \infty$, the second factor will converge to one, and the third to the exponent function:

$$  
\lim _{r \rightarrow \infty} f(k ; r, p)=\frac{\lambda^k}{k !} \cdot 1 \cdot \frac{1}{e^\lambda}  
$$

  

which is the mass function of a Poisson-distributed random variable with expected value \lambda. In other words, the alternatively parameterized negative binomial distribution converges to the Poisson distribution and r controls the deviation from the Poisson. This makes the negative binomial distribution suitable as a robust alternative to the Poisson, which approaches the Poisson for large r, but which has larger variance than the Poisson for small r.

$$  
\operatorname{Poisson}(\lambda)=\lim _{r \rightarrow \infty} \mathrm{NB}\left(r, \frac{r}{r+\lambda}\right) .  
$$

  

计算该概率密度的函数放在/src_test/nochangepoints.py中
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTc1MTUyMDY4LC04NDI1NDU0NzMsLTE1ND
I0MTgzMTJdfQ==
-->